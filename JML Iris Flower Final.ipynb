{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm uses data from four features to classify flower into one of three types of Iris Flowers. The four features are Sepal Length, Sepal Width, Petal Length, and Petal Width. The three types of Iris Flowers are 'Iris-setosa', 'Iris-versicolor', and 'Iris-virginica'. The discussion on how the algorithm makes the predictions is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2444.5871389198182 0.5703703703703704\n",
      "1622.4090943649549 0.9481481481481482\n",
      "1856.3319365267225 0.9555555555555556\n",
      "1792.9960091449575 0.9629629629629629\n",
      "1906.4546639762107 0.9481481481481482\n",
      "1850.0546451676003 0.9703703703703703\n",
      "1712.7049218295235 0.9333333333333333\n",
      "1938.9666561312604 0.9703703703703703\n",
      "1904.7311487906939 0.9555555555555556\n",
      "1953.0744634253265 0.9555555555555556\n",
      " \n",
      "Final Cost and Accuracy of training data: \n",
      "1953.0744634253265 0.9555555555555556\n",
      " \n",
      "Cost and Accuracy of testing data: \n",
      "1953.0744634253265 0.9444444444444444\n",
      " \n",
      "Testing the algorithm's classifier: \n",
      "Iris-versicolor\n",
      "Iris-virginica\n",
      "Iris-setosa\n",
      "\n",
      "Weights of input layer and hidden layer: \n",
      "[[ -4269.26426116  -8419.98151452 -13548.78898352   1993.66912437\n",
      "  -13471.40036771 -12947.26553449 -11204.66468377  25322.69167123\n",
      "   25976.19513477  17427.26294486  -7035.94503569  23112.30343406\n",
      "   20050.62461375  13697.22619701  -3453.66009847  -6133.42453313\n",
      "    8749.09394242   8257.41960608  17914.92991823  19569.7719509\n",
      "   22988.76496123  -9468.06855707  21388.35151493  17208.53670859\n",
      "    3270.40943188  -6161.97401286  18787.27820071  -1788.74627464\n",
      "   -6899.41568398  25749.27766008  22451.01923787  -7855.04530426\n",
      "   -5614.14763837 -10203.29111594   2808.71134394   2241.62066032\n",
      "    1908.84664641  19500.60721207  26309.10031827  22796.37264666\n",
      "   17809.67516571  -3794.49496368  23809.07639455  29932.3512126\n",
      "   14067.3252856   20142.02402287   -731.77848971   1364.78223035\n",
      "  -16404.43259019  22780.96635272  19371.68516596 -10917.02924321\n",
      "    3412.11909855 -10254.30131311  23925.16653221  -2042.05682264\n",
      "    -376.40054905   1100.60527465   3797.21945302   1426.44139263\n",
      "   15504.23046251  22515.81089695  17665.27297159  -9865.60767276\n",
      "   14058.3746432   -7923.48364964  24993.80062306  -9993.12518951\n",
      "   -3807.54104301    716.31884028  21362.69205516 -11353.68629219\n",
      "  -10722.24307474  -7174.5660958  -10366.68821573  22822.80273427\n",
      "   19153.20615013   -270.45876445  -9590.1755045   -6300.04539149\n",
      "   28104.94125054  -1397.04234939  20618.6206917    8406.51419381\n",
      "    2449.79524086  16196.17419043  19850.14374545  -2825.55841414\n",
      "   18814.04619821  -6648.24436538   1037.07505902  -8571.80605269\n",
      "   11227.70519856  21852.16841791  -4396.43459815   2174.29209962\n",
      "    7472.92900426     32.15852328    283.56903231  -9575.60654837]\n",
      " [  2869.05923999   4125.3988517    7851.45008827    181.45968468\n",
      "   14373.21465197   6411.58661107   6250.36782686  -8648.77821625\n",
      "   -8811.64779303  -4590.13033166   4075.43018991  -7112.0290796\n",
      "   -6009.58659137  -1336.2841255    4338.55650764   2746.15488156\n",
      "   -1911.23618454  -2953.1417822   -6723.39604594  -7723.8277627\n",
      "   -8684.22684388   4970.68828786  -6548.17137173  -4499.63797799\n",
      "    -414.35607019   3060.2881566   -6623.39695691   1262.66077215\n",
      "    4075.9542956   -8799.91773269  -6608.0075738    4359.04015038\n",
      "    2396.43226032   4837.55047149   -730.11004333   -557.55772546\n",
      "     -61.21108957  -6965.66814803 -10038.2588162   -6366.27835474\n",
      "   -6978.79651824   2279.86622358  -9879.4086687  -10676.14773389\n",
      "   -4610.8560091   -7013.02747165   2153.04328449   -357.69144642\n",
      "    8471.664385    -5537.4378973   -7395.63906304   4863.87988521\n",
      "    -446.95328951   5797.12273291  -6280.29569103   5997.99751261\n",
      "    1826.39261573   1517.55821205    652.86090959   1380.97327505\n",
      "   -5486.76295938  -7040.86346856  -6539.66211447   4858.05192506\n",
      "   -3822.68610946   4751.18692043  -8474.80599776   5007.00661606\n",
      "    2921.05210909   1285.79842919  -6469.37019732   5718.19592914\n",
      "    4728.05189819   3789.10055353   4999.66790393  -9128.33719065\n",
      "   -7226.6103207     496.12843217   4529.20369593   3730.08425333\n",
      "  -10443.12935246    707.31346257  -6925.57473595  -1972.25341911\n",
      "    -721.54340557  -4154.48654307  -7487.02218049   2890.16290593\n",
      "   -7157.21323174   3177.41209023    969.22685177   3886.45150943\n",
      "   -3064.66836414  -8497.36968258   1867.38068225   -312.47915456\n",
      "   -2799.92668906   3368.58252193   1892.25082088   4297.92375807]\n",
      " [ -5157.08625543  -9515.95860099 -15796.8837007    1774.04212013\n",
      "  -18525.1443951  -14613.8044878  -12982.34993267  26935.3784211\n",
      "   27598.28061259  17960.1641614   -8232.26092431  24271.41881766\n",
      "   21017.19695868  13118.15225371  -5013.69342576  -6817.16079501\n",
      "    8861.68884435   8811.22248541  19308.36435523  21250.27306142\n",
      "   24849.38486406 -10840.65860365  22474.27664748  17763.04401315\n",
      "    3160.58150072  -6995.44729743  20092.59562808  -2187.80248494\n",
      "   -8112.96019188  27391.71486416  23465.05073611  -9104.10474287\n",
      "   -6199.59826646 -11459.08354348   2914.43905857   2275.30805188\n",
      "    1768.54539889  20856.71976217  28420.96762806  23681.97411388\n",
      "   19316.83014662  -4493.69169339  26057.27377166  32010.7484407\n",
      "   14925.99047756  21493.69372501  -1586.81815029   1409.15078525\n",
      "  -18668.54407654  23272.91270693  20988.91525026 -12089.89841421\n",
      "    3291.60928752 -11917.20437115  24647.13758428  -4465.07574382\n",
      "   -1114.72576698    379.52586342   3225.58623665    731.84306273\n",
      "   16589.7352588   23699.47143648  18996.500662   -11125.35686142\n",
      "   14537.43081171  -9341.48965931  26583.74113307 -11336.49185745\n",
      "   -4747.55496805    124.50999213  22413.91949893 -12894.21615157\n",
      "  -11892.49543291  -8235.56701937 -11683.43533448  24839.30393843\n",
      "   20655.71851412   -475.48118238 -10766.95571424  -7410.79480914\n",
      "   30244.42756948  -1626.92986733  21919.80969292   8580.94380553\n",
      "    2540.60469086  16677.17532399  21438.0137657   -3836.32673018\n",
      "   20310.16724833  -7487.74390279    529.28556211  -9559.92187115\n",
      "   11647.70466432  23671.40913543  -4826.54359978   2119.62063982\n",
      "    8031.12150464  -1391.31289983   -534.93456647 -10649.6399249 ]\n",
      " [ -4883.42799536  -9114.34014529 -15112.80802044   1828.12273493\n",
      "  -17357.94971479 -14017.18184753 -12426.62284027  26486.70533632\n",
      "   27118.87273231  17766.3052461   -7864.90638613  23949.59101422\n",
      "   20773.88601829  13119.48070525  -4640.32432631  -6547.44047494\n",
      "    8845.83030442   8746.71011722  19047.27166475  20911.2504503\n",
      "   24406.60046889 -10359.61399602  22149.13747244  17634.81243802\n",
      "    3242.55065513  -6694.47640393  19779.04303638  -2041.75540567\n",
      "   -7741.01669016  26948.57513408  23176.16388268  -8707.67564866\n",
      "   -5968.79837776 -11019.78653305   2948.35160919   2270.43016817\n",
      "    1805.82168618  20572.19882372  27853.39701345  23375.63263637\n",
      "   19037.43159591  -4272.20864758  25543.2111903   31395.44287527\n",
      "   14824.95687852  21212.39010543  -1385.91286115   1432.62538732\n",
      "  -17904.67152943  23034.99763268  20631.49553618 -11612.65224422\n",
      "    3368.43867259 -11410.7090284   24363.36133129  -3943.54839733\n",
      "    -938.64673264    535.70783568   3324.09179442    876.76921425\n",
      "   16410.83125654  23380.07820207  18745.11909625 -10639.57677664\n",
      "   14396.32286945  -8930.88988128  26167.68520015 -10867.45629533\n",
      "   -4481.01294877    263.5678714   22136.80642568 -12387.35360525\n",
      "  -11456.92612704  -7845.78404363 -11216.97826237  24391.02194502\n",
      "   20360.73897399   -418.95682088 -10336.64536355  -7082.42400059\n",
      "   29694.70751309  -1527.7203318   21590.25518902   8513.61566418\n",
      "    2575.84183706  16575.67587096  21056.58385348  -3570.74422536\n",
      "   20011.99479159  -7184.31970774    643.92588788  -9183.96332802\n",
      "   11534.34529836  23273.47366766  -4577.05879154   2142.63406009\n",
      "    7971.48729799  -1076.43056084   -354.50802756 -10236.85058856]]\n",
      "\n",
      "Biased Terms of the first hidden layer:\n",
      "[ -9007.90287222 -10764.1437754  -12270.84744941  -9166.60791688\n",
      " -22671.09515976 -11046.3431381   -9671.04703482 -26190.92481397\n",
      " -24338.30385734 -22876.13306693 -10316.50488332 -26392.95400825\n",
      " -27134.79489718 -20996.35425411 -10872.3878051   -8909.11039203\n",
      " -17144.12615004 -17675.84266907 -27603.62158282 -26868.4592648\n",
      " -25996.00225815 -13221.44410952 -24121.19034946 -26641.52677137\n",
      " -12620.38304855  -8922.07244708 -24993.87316357  -7772.1539043\n",
      "  -8948.52699566 -27632.08914128 -26907.36191469 -10119.52448791\n",
      "  -6488.96330385  -9965.30773447 -11144.11048849  -6589.04106894\n",
      "  -8510.9070835  -27543.46185993 -22716.17793889 -24445.62641759\n",
      " -26899.31366358  -8474.47637204 -25121.34570027 -25307.24338335\n",
      " -24183.1050707  -27487.15678183  -9649.94266638  -7632.43932359\n",
      " -14448.06732361 -25580.57930825 -25135.64445336 -10315.969036\n",
      " -12496.92516971 -11271.2654901  -26760.03563175 -17700.37129721\n",
      "  -9512.42629537 -10288.82042805 -11529.10369436 -10365.8531425\n",
      " -26167.27082431 -26844.73984518 -27037.74404236 -12798.81589691\n",
      " -20201.25654271  -8927.60248789 -27551.84103195  -9638.86754875\n",
      "  -8357.0458129   -9693.90571817 -28046.09152484 -11308.35023243\n",
      " -10235.6028938  -13442.57895867 -11108.89861424 -25575.35577128\n",
      " -27437.2466924   -3428.11355776  -9876.1079718   -8346.68691891\n",
      " -28972.09684483  -6223.1879116  -25917.95185925 -14271.97560928\n",
      " -11428.65663504 -25545.17994781 -24592.90861993  -9282.02821171\n",
      " -26404.18965648  -7636.0255824   -8648.30956521  -9663.44799204\n",
      " -17236.47839303 -27181.83272939 -10242.23505062  -8622.2206673\n",
      " -16728.43922486 -14030.31536388 -10002.67284668  -9569.4120033 ]\n",
      "\n",
      "Weights of hidden layer and output layer:\n",
      "[[ 8.22643122e-01 -1.28256277e-01 -1.55730862e+00]\n",
      " [ 4.74913695e-02 -5.65493643e-01  1.41450891e+00]\n",
      " [-1.55326005e+00  4.10972055e-02 -1.00885680e+00]\n",
      " [ 3.09399827e+00  3.79507389e-01 -5.94712075e-01]\n",
      " [ 1.40752515e+00 -1.26032458e+00 -8.61106838e-01]\n",
      " [-1.98349312e+00  1.32207743e+00 -7.78168410e-01]\n",
      " [-9.45818157e-01 -4.60787978e-01 -6.06135223e-01]\n",
      " [ 8.38253037e-01  5.55240143e-02 -4.05189981e-01]\n",
      " [ 1.85248903e+00  8.99204126e-01 -1.12281661e+00]\n",
      " [ 1.38545882e+00  7.11375633e-02 -1.51840677e+00]\n",
      " [ 2.23343883e+00  1.87800482e-01  7.43780376e-01]\n",
      " [-6.37845861e-02  1.23274306e+00 -1.74647897e-01]\n",
      " [ 7.04630040e-01  7.89036818e-01  8.15507637e-01]\n",
      " [ 1.95355849e+00  6.03128517e-01 -1.99970829e+00]\n",
      " [ 1.67828340e+00  9.14732546e-02 -1.91110311e+00]\n",
      " [ 1.64221354e+00 -8.58793843e-01 -2.47454789e-01]\n",
      " [-4.00589860e-01  2.05151373e+00 -1.10988190e+00]\n",
      " [-4.82998669e-01 -3.09741839e-01  1.86287810e+00]\n",
      " [-7.84612239e-01 -1.19236586e+00  1.35187276e+00]\n",
      " [-1.08348794e+00 -2.29542741e-01  8.15556905e-01]\n",
      " [-3.55051874e-01  6.75281583e-02 -1.15517734e-02]\n",
      " [-7.86312125e-01  4.52652531e-01  7.64453529e-01]\n",
      " [ 2.11396445e+00  6.46867381e-01  3.25628582e-01]\n",
      " [-4.65245794e-01 -6.98984449e-01  1.31442114e+00]\n",
      " [-9.52235135e-01  4.17954367e-01  1.78281606e+00]\n",
      " [-3.81681684e-01 -1.81096346e+00 -1.64097421e+00]\n",
      " [-1.85354647e-01 -2.15638468e-03  4.59126871e-01]\n",
      " [ 1.23820394e+00 -7.94633449e-01 -1.65118066e+00]\n",
      " [ 1.78042466e+00  9.86380305e-01  4.84391055e-01]\n",
      " [ 7.94335180e-01  1.03421056e+00 -2.30245092e-03]\n",
      " [-3.22665707e-02  5.01335253e-01 -3.59064626e-01]\n",
      " [ 1.68773565e-01 -1.55381750e+00 -5.70794291e-01]\n",
      " [ 4.19524589e-01 -2.64240599e+00 -5.93037871e-01]\n",
      " [ 2.19026847e-02 -8.58820290e-01 -8.43074191e-01]\n",
      " [-1.04307516e+00 -2.00726901e+00  1.70021923e+00]\n",
      " [ 1.01106589e+00 -2.04132493e+00  7.45449909e-01]\n",
      " [ 1.93980506e+00  9.00481495e-01 -1.19156073e+00]\n",
      " [-1.38952717e+00  8.90781634e-02  5.03180070e-01]\n",
      " [-1.99782589e-01 -6.59591729e-01 -1.41739806e-02]\n",
      " [ 1.85185684e+00  2.74802826e-01 -2.81006019e-01]\n",
      " [-4.15215996e-01 -1.88753259e-01  1.63554161e+00]\n",
      " [ 2.85106357e+00 -2.99548368e-01  8.88488280e-01]\n",
      " [-5.54273401e-01  9.92716237e-01  8.99743670e-01]\n",
      " [ 1.05174568e+00  1.91518876e+00 -7.42953581e-01]\n",
      " [-1.18714073e+00 -4.23621868e+00  5.26540304e+00]\n",
      " [-1.36877005e+00  8.11189492e-01  4.96355660e-01]\n",
      " [ 8.78892711e-01 -3.34219271e-01 -2.39523181e+00]\n",
      " [ 1.31924483e+00 -5.67020124e-01 -2.03567137e+00]\n",
      " [-3.00364480e+00 -1.82452081e-01 -1.01619542e+00]\n",
      " [ 1.80345422e+00  3.97522490e-01 -8.17297856e-01]\n",
      " [-3.97078668e-01 -6.70346299e-01  3.90543461e-01]\n",
      " [-1.51912335e+00  1.39928904e+00 -4.66940321e-01]\n",
      " [-1.21247126e+00  4.31616667e-03  1.58589318e+00]\n",
      " [ 7.15038275e-01  1.26394100e-01  2.41328550e-01]\n",
      " [ 1.44652680e+00 -1.23865378e-01 -1.02566717e+00]\n",
      " [ 4.05456175e+00 -2.93730555e-01  9.53758451e-01]\n",
      " [ 1.31646050e+00 -9.89555178e-01 -2.10389315e+00]\n",
      " [ 2.89990273e+00  7.96965253e-01 -8.27834539e-01]\n",
      " [ 1.94782284e+00 -1.84903152e-01 -7.34705982e-01]\n",
      " [ 2.27578252e+00 -4.51802164e-02 -1.46149210e+00]\n",
      " [-1.03632819e+00 -1.71606470e+00  1.78954232e+00]\n",
      " [ 2.26362681e-01  7.92865267e-01  4.72726233e-02]\n",
      " [-3.32374202e-01 -3.72827109e-01  1.81820665e+00]\n",
      " [-3.07517204e+00  5.51242572e-01 -8.97163595e-01]\n",
      " [ 1.93133538e+00  4.68690718e-01 -1.43482900e-01]\n",
      " [ 2.06904252e+00  2.82576643e-01  7.50331674e-01]\n",
      " [-4.79071162e-01  1.09269253e+00 -7.67749160e-01]\n",
      " [-4.70027527e-01 -1.13140829e+00  6.26386773e-01]\n",
      " [ 9.51570648e-01 -7.60539550e-01 -1.81766572e+00]\n",
      " [ 2.19546694e+00 -6.84381915e-01 -1.45688057e+00]\n",
      " [ 7.04019668e-01  6.17960753e-01  8.08532870e-01]\n",
      " [-7.01504392e-01 -2.10512404e+00 -7.18710659e-01]\n",
      " [ 1.08202459e+00 -6.47939720e-01  5.53568588e-01]\n",
      " [-2.79422357e+00  3.03376874e-02  7.20668306e-01]\n",
      " [ 9.80529947e-01  1.42964645e-01  8.89737673e-01]\n",
      " [-8.66826411e-01  1.20391459e+00  5.48905229e-01]\n",
      " [-1.33890921e+00 -1.20685849e+00  6.98840871e-01]\n",
      " [ 1.44650937e+00 -2.76679407e-01 -7.12288389e-01]\n",
      " [-8.80663827e-01  2.83661708e-01  3.25771180e-01]\n",
      " [ 9.35113585e-01 -1.25259720e+00 -4.95252677e-01]\n",
      " [-2.46738048e-02  1.71459315e+00 -7.30869602e-01]\n",
      " [ 1.77473928e+00 -3.18281370e-01 -9.23982249e-01]\n",
      " [-1.31698802e+00 -1.38065936e+00 -8.27247648e-01]\n",
      " [ 2.02410856e+00 -4.29410187e-01  5.66429273e-01]\n",
      " [-1.09461625e-01  2.32210301e-01  2.37646358e+00]\n",
      " [-4.43606456e-01 -9.64098484e-01  2.25072111e+00]\n",
      " [-8.15621818e-01 -6.75280103e-01 -1.07754596e-01]\n",
      " [-1.07790565e-01 -1.07426423e+00 -3.04494942e+00]\n",
      " [-1.06278168e+00  9.16991189e-02  8.55932999e-01]\n",
      " [ 1.12760306e+00 -5.16407502e-01  3.50105334e-03]\n",
      " [ 8.55342724e-01  2.49202387e-01 -2.49981720e+00]\n",
      " [ 7.72285751e-01 -4.79630894e-01 -1.18101370e-02]\n",
      " [ 1.65791654e+00 -1.75593332e+00  5.46955626e-01]\n",
      " [-1.19094525e+00  5.19382632e-01  5.36966475e-01]\n",
      " [-1.60151161e+00  3.99504275e+00  8.36379922e-01]\n",
      " [ 8.15201205e-01  5.22229158e-01 -1.65146941e+00]\n",
      " [-8.41595372e-01 -1.73093841e+00  2.53739451e+00]\n",
      " [ 1.97806415e+00 -9.70406630e-01 -1.54682995e+00]\n",
      " [ 2.31182253e+00 -3.45188935e-01 -1.34812805e+00]\n",
      " [ 8.66128572e-02 -1.80496885e+00  1.30788091e+00]]\n",
      "\n",
      "Biased Terms of the output layer:\n",
      "[-3.20790401  2.97505954  0.36360396]\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Iris.csv\") #loading the data into Python\n",
    "df.head()#checking the contents to manipulate\n",
    "\n",
    "#Preprocessing the data (making the features and the targets)\n",
    "\n",
    "X = np.asmatrix(np.copy(df))[:,:5] #getting all the columns of the feature data\n",
    "X = np.delete(X, 0, axis = 1) #dropping column of index 0 because it is \"id\".\n",
    "\n",
    "nameOfTargets = df.Species.unique() #Getting the unique values of the target column for one hot encoding\n",
    "\n",
    "Y_data = [] #Empty list that will eventually become target data\n",
    "for i in df.iloc[:,5]: #Iterating through all the targets\n",
    "    for j in range(nameOfTargets.shape[0]): #for j from 0 to N, where N is the number of items in nameOfTargets\n",
    "        if i == nameOfTargets[j]:       \n",
    "            Y_data.append(j)\n",
    "    #This code segment iterates through the targets (which are Strings), and converts it into 0, 1 or 2.\n",
    "        #The index number of the item in nameOfTargets is how they will be represented \n",
    "        #in the target data. I.E if the value of the target is equal to the FIRST \n",
    "        #item of nameOfTargets, the value is represented by the item's INDEX (0).\n",
    "        \n",
    "N = len(Y_data) #Getting the number of items in the dataset\n",
    "Y = np.zeros(N*nameOfTargets.shape[0]).reshape(N,nameOfTargets.shape[0]) \n",
    "    #Making the target matrix. The number of rows = number of subjects, number of columns = number of unique targets\n",
    "\n",
    "\n",
    "for i in range(N): #One Hot Encoding. After the loop finishes, Y will be the final target matrix.\n",
    "    t = Y_data[i]\n",
    "    Y[i,t] = 1\n",
    "    \n",
    "    \n",
    "#Standardizing values in the feature matrix X\n",
    "for i in range(X.shape[1]):\n",
    "    X[:,i] = (X[:,i].astype(float) - np.mean(X[:,i].astype(float)))/np.std(X[:,i].astype(float))\n",
    "    \n",
    "X_new = np.asmatrix(np.copy(df))[:,:5]  \n",
    "X_new = np.delete(X_new, 0, axis = 1)\n",
    "#Making a copy of the feature data that is not standardized. This data will be used later in the algorithm's classifier\n",
    "    #to standardize the input data.\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.4, random_state = 10) #Splitting the data into testing data and training data\n",
    "\n",
    "\n",
    "\n",
    "#Deep Learning\n",
    "\n",
    "np.random.seed(1) #making sure the weights are the same every time the cell is rerun (still random)\n",
    "N,D = X_train.shape #N = num subjects, D = num features\n",
    "M = 100 #num hidden nodes of the hidden layer\n",
    "K = nameOfTargets.shape[0] #number of outputs\n",
    "\n",
    "iteration_num = 1000 #Number of times gradient descent will be performed\n",
    "a = 0.02 #learning rate\n",
    "\n",
    "#creating the weights (randomly)\n",
    "W = np.random.randn(D*M).reshape(D,M)\n",
    "V = np.random.randn(M*K).reshape(M,K)\n",
    "\n",
    "\n",
    "#creating the biased terms\n",
    "b = np.random.randn(M).reshape(1,M) #Generating biased terms for hidden nodes\n",
    "b_ones = np.ones(N).reshape(N,1) #Making an Nx1 matrix to multiply with the biased terms.\n",
    "b = np.dot(b_ones,b) #After multiplying, NxM matrix (each row is a subject, each column is a node). The biased terms of each column are the same (biased term of each node does not change for the subject).\n",
    "\n",
    "c = np.random.randn(K).reshape(1,K) #Repeating the process for the biased terms of the output layer. NxK matrix once completed.\n",
    "c_ones = np.ones(N).reshape(N,1)\n",
    "c = np.dot(c_ones, c)\n",
    "\n",
    "for j in range(iteration_num): #Back Propagation\n",
    "    \n",
    "    #feed forward\n",
    "    z = np.dot(X_train,W) + b\n",
    "    z = 1/(1 + np.exp(-z.astype(float)))\n",
    "    predictions = np.exp(np.dot(z,V) + c)\n",
    "\n",
    "    #softmax\n",
    "    for i in range(predictions.shape[0]):\n",
    "        predictions[i,:] = predictions[i,:]/np.sum(predictions[i,:])\n",
    "\n",
    "\n",
    "    #gradient descent (all from formula)\n",
    "    \n",
    "        #Calculating all the partial derivatives of cost\n",
    "    dV = np.dot(z.T,(y_train - predictions)) \n",
    "    dZ = np.dot(np.dot(np.dot((y_train - predictions), V.T).T, z),(1-z.T)) #will be used to calculate dW\n",
    "    dW = np.dot(X_train.T,dZ.T) \n",
    "    db = np.dot(np.dot(np.dot((y_train - predictions), V.T).T, z), (1-z.T)).T.sum(axis = 0) \n",
    "    dc = (y_train - predictions).sum(axis = 0) \n",
    "    \n",
    "    \n",
    "    W += a*dW.astype(float) #Gradient Descent\n",
    "    V += a*dV.astype(float) \n",
    "    b += a*db.astype(float)\n",
    "    c += a*dc.astype(float)\n",
    "    \n",
    "    if j%100 == 0: #Every 100 iterations, print out the cost and accuracy\n",
    "        total = -np.dot(y_train.T, np.log(predictions))\n",
    "        cost = total.sum() #Cost of the model\n",
    "        Accuracy = np.mean(np.round(predictions) == y_train) #Accuracy of the model\n",
    "        print(cost, Accuracy)\n",
    "        \n",
    "        \n",
    "print(\" \")\n",
    "print(\"Final Cost and Accuracy of training data: \")\n",
    "print(cost, Accuracy)\n",
    "        \n",
    "#Applying the model to the test data. The X_test data must be put through the softmax function and compared to y_test\n",
    "\n",
    "#feed forward\n",
    "z = np.dot(X_test,W) + b[0] #using only b causes a dimension error. Using b[0] gets all the relevant biased terms and makes use of how numpy array addition works.\n",
    "z = 1/(1 + np.exp(-z.astype(float)))\n",
    "test_predictions = np.exp(np.dot(z,V) + c[0]) #same thing with c[0]\n",
    "\n",
    "#softmax\n",
    "for i in range(test_predictions.shape[0]):\n",
    "    test_predictions[i,:] = test_predictions[i,:]/np.sum(test_predictions[i,:])\n",
    "    \n",
    "test_Acc = np.mean(np.round(test_predictions) == y_test)\n",
    "test_total = -np.dot(y_test.T, np.log(test_predictions))\n",
    "test_cost = total.sum()\n",
    "\n",
    "print(\" \")\n",
    "print(\"Cost and Accuracy of testing data: \")\n",
    "print(test_cost, test_Acc)\n",
    "\n",
    "#Function to classify the flower. Parameters are the Sepal Length, Sepal Width, Petal Length and Petal Width. (Classifier)\n",
    "def classify(SLen, SWid, PLen, PWid):\n",
    "    \n",
    "    data = np.matrix([SLen,SWid, PLen, PWid]) #Converting inputs into matrix\n",
    "    \n",
    "    for i in range(X_new.shape[1]):\n",
    "        data[:,i] = (data[:,i].astype(float) - np.mean(X_new[:,i].astype(float)))/np.std(X_new[:,i].astype(float)) \n",
    "        #Standardizing inputs using X_new, which was created earlier in the algorithm\n",
    "    \n",
    "    #Passing the inputs through feed forward\n",
    "    z = np.dot(data,W) + b[0]\n",
    "    z = 1/(1 + np.exp(-z.astype(float)))\n",
    "    test_predictions = np.exp(np.dot(z,V) + c[0])\n",
    "\n",
    "    #softmax\n",
    "    for i in range(test_predictions.shape[0]):\n",
    "        test_predictions[i,:] = test_predictions[i,:]/np.sum(test_predictions[i,:])\n",
    "    \n",
    "    test_predictions = np.round(test_predictions) #rounding the predictions to only get 1 and 0\n",
    "    j = np.where(test_predictions == 1)[1][0] #Getting the index of which target we classified the data as (which target node has a value of 1)\n",
    "    \n",
    "    \n",
    "    return nameOfTargets[j]#Returning the name of the target\n",
    "\n",
    "print(\" \")\n",
    "print(\"Testing the algorithm's classifier: \") #The classifier should return the following.\n",
    "print(classify(5.6,3,4.5,1.5)) #Versicolor\n",
    "print(classify(6.7,3.3,5.7,2.5)) #Virginica\n",
    "print(classify(5.1,3.5,1.4,0.2)) #Setosa\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Weights of input layer and hidden layer: \")\n",
    "print(W)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Biased Terms of the first hidden layer:\")\n",
    "print(b[0])\n",
    "\n",
    "print(\"\")\n",
    "print(\"Weights of hidden layer and output layer:\")\n",
    "print(V)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Biased Terms of the output layer:\")\n",
    "print(c[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the algorithm makes predictions:\n",
    "- First, the algorithm is trained using existing data. The Iris Flower data set is split into testing and training data, and the training data is standardized and inputted into the model. \n",
    "- The model, which has 1 hidden layer containing 100 nodes (M), finds the number of subjects (N), the number of features (D), and the number of outputs (K). All the weights and biased terms between the layers are then randomly generated according to these numbers.\n",
    "- Backpropagation is now performed. First, the feed forward function. The algorithm multiplies the training data with the weights between the input layer and the hidden layer (W), and then adds the bias terms of each node (b) to the computed values (z). These computed values are passed through the sigmoid function (Z). \n",
    "- These Z values are then multipled by the weights between the hidden layer and the output layer (V), then the biased terms of the output layer are added (c). These values are then put through the softmax function. \n",
    "- The cost derivatives (dW, dV)are then calculated and gradient descent is performed on all the weights and biased terms in the model with a learning rate of 0.02. \n",
    "- This entire process of backpropagation is repeated for 1000 iterations. Once completed, the weights and biased terms can be considered ideal. The model has finished training.\n",
    "- To predict using the model, pass the values of the sepal length, sepal width, petal length and petal width respectively into the classifier. The algorithm will use feed forward on the data and classify it into one of the three targets using the ideal weights and biased terms. Of the three target nodes, the node with the highest value will be what we classify the flower as. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
