{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression():\n",
    "    def __init__(self):\n",
    "        #initializing the class (like import numpy as np)\n",
    "        #How to code in Jupyter: LR = Logistic_Regression()\n",
    "        print(\"LR class activated\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    def standardize_values(self, mat, column_index_list):\n",
    "        #standardization method\n",
    "        #input the data and then a list containing the column indexes to be standardized\n",
    "        for i in range(len(column_index_list)):\n",
    "            mat[:,column_index_list[i]] = (mat[:,column_index_list[i]] - mat[:,column_index_list[i]].mean())/mat[:,column_index_list[i]].std()\n",
    "        return mat\n",
    "    \n",
    "        \n",
    "        \n",
    "    def OHE_from_data(self, data, column_index_list):\n",
    "        #One Hot Encoding directly from data\n",
    "        \"\"\"\n",
    "        Column Indexes in list MUST be in descending order\n",
    "        \"\"\"\n",
    "        mat = np.asmatrix(pd.read_csv(data)) #reads data as matrix\n",
    "        for i in range(len(column_index_list)):\n",
    "            num_of_categories = max(mat[:,column_index_list[i]]) #Finding number of OHE columns to add\n",
    "            N,D = mat.shape\n",
    "            zeros = np.zeros(N*int(num_of_categories)).reshape(N,int(num_of_categories)) #Making OHE columns of 0\n",
    "            ohe_mat = np.hstack((mat,zeros)) #Concatenating the data\n",
    "        \n",
    "            for num in range(N):\n",
    "                t = int(ohe_mat[num,column_index_list[i]])\n",
    "                ohe_mat[num, (D - 1) + t] = 1 \n",
    "            ohe_mat = np.delete(ohe_mat, column_index_list[i], axis = 1) #dropping the original categorical column\n",
    "        \n",
    "        return ohe_mat.astype(float)\n",
    "        \n",
    "    \n",
    "    def OHE_from_mat(self, mat, column_index_list):\n",
    "        #One Hot Encoding if a matrix is used instead\n",
    "        \"\"\"\n",
    "        Column Indexes in list MUST be in descending order\n",
    "        \"\"\"\n",
    "        for i in range(len(column_index_list)):\n",
    "            num_of_categories = max(mat[:,column_index_list[i]]) #Finding number of OHE columns to add\n",
    "            N,D = mat.shape\n",
    "            zeros = np.zeros(N*int(num_of_categories)).reshape(N,int(num_of_categories)) #Making OHE columns of 0\n",
    "            ohe_mat = np.hstack((mat,zeros)) #Concatenating the data\n",
    "        \n",
    "            for num in range(N):\n",
    "                t = int(ohe_mat[num,column_index_list[i]])\n",
    "                ohe_mat[num, (D - 1) + t] = 1 \n",
    "            ohe_mat = np.delete(ohe_mat, column_index_list[i], axis = 1) #dropping the original categorical column\n",
    "\n",
    "        return ohe_mat.astype(float)\n",
    "    \n",
    "    def auto_z_make(self, mat):\n",
    "        #Method to make Z; the method creates the weights for you\n",
    "        N,D = mat.shape\n",
    "        W = np.random.randn(D)\n",
    "        return np.dot(W,mat.T)\n",
    "    \n",
    "    def z_make(self, mat,W):\n",
    "        #making Z using your own weights\n",
    "        return np.dot(W,mat.T)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        #predictions aka sigmoid curve\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def cost_function(self, T, Q):\n",
    "        #T = Targets; Q = Sigma = Predictions\n",
    "        return np.dot(-(T, np.log(Q).T) + np.dot((1-T),np.log(1-Q).T))\n",
    "    \n",
    "    def auto_optimize_weights(self, X,Y,iter_num,a,lamb):\n",
    "        #Gradient Descent and Cost Regularlization. Computer comes up with Weights\n",
    "        N,D = X.shape\n",
    "        W = np.random.randn(D) #Making the weights\n",
    "        predictions = self.sigmoid(np.dot(W,X.T))\n",
    "        print(np.mean(np.round(predictions) == Y)) #initial accuracy of model (might remove later)\n",
    "    \n",
    "        for i in range(iter_num):\n",
    "            predictions = self.sigmoid(np.dot(W,X.T))\n",
    "            derivative = np.dot((predictions - Y), X) + lamb*W\n",
    "            W -= a*derivative.astype(float) #Gradient Descent\n",
    "        \n",
    "        print(np.mean(np.round(predictions) == Y)) \n",
    "        return W\n",
    "    \n",
    "    def optimize_weights(self, X,Y,iter_num,a,lamb,W):\n",
    "        #Gradient Descent using your own Weights\n",
    "        predictions = self.sigmoid(np.dot(W,X.T))\n",
    "        print(np.mean(np.round(predictions) == Y)) #initial accuracy\n",
    "    \n",
    "        for i in range(iter_num):\n",
    "            predictions = self.sigmoid(np.dot(W,X.T))\n",
    "            derivative = np.dot((predictions - Y), X) + lamb*W\n",
    "            W -= (a*derivative).astype(float) #Gradient Descent\n",
    "        \n",
    "        print(np.mean(np.round(predictions) == Y))\n",
    "        return W\n",
    "    \n",
    "    def accuracy_check(self, predictions, targets):\n",
    "        return np.mean(np.round(predictions) == targets)\n",
    "\n",
    "    def remove_NaN_Numerical(self, mat,column_index_list):\n",
    "        N,D = mat.shape\n",
    "        for num in range(len(column_index_list)):\n",
    "            temporary_mat = mat[:,column_index_list[num]].astype(float)\n",
    "            mean = temporary_mat[~np.isnan(temporary_mat)].mean()\n",
    "            for i in range(N):\n",
    "                if np.isnan(mat[i,column_index_list[num]]):\n",
    "                    mat[i,column_index_list[num]] = mean\n",
    "        return mat\n",
    "\n",
    "    def remove_NaN_Categorical(self, mat,column_index_list):\n",
    "        N,D = mat.shape\n",
    "        for num in range(len(column_index_list)):\n",
    "            temporary_mat = mat[:,column_index_list[num]].astype(float)\n",
    "            mean = temporary_mat[~np.isnan(temporary_mat)].mean()\n",
    "            for i in range(N):\n",
    "                if np.isnan(mat[i,column_index_list[num]]):\n",
    "                    mat[i,column_index_list[num]] = np.round(mean)\n",
    "        return mat\n",
    "    \n",
    "    def remove_NaN_Pandas(self, df):\n",
    "        \"\"\"\n",
    "        Remove NaN values while still in DataFrame (df must be a dataframe).\n",
    "        Does not work for categorical columns\n",
    "        \"\"\"\n",
    "        df.fillna(df.mean(), inplace = True)\n",
    "        #fillna = fill NaN. inplace = True to permanantly change the data\n",
    "    \n",
    "    \n",
    "    def make_model(self, X_train, X_test, y_train, y_test, nameOfTargets, M = 100, iteration_num = 1000, a = 0.02,):\n",
    "        \"\"\"\n",
    "        default: num nodes = 100, num iterations = 1000, learning rate = 0.02\n",
    "        for nameOfTarets, use df.Species.unique() to get all unique categories. Drop as necessary.\n",
    "        \"\"\"\n",
    "        np.random.seed(1) #making sure the weights are the same every time the cell is rerun (still random)\n",
    "        N,D = X_train.shape #N = num subjects, D = num features\n",
    "        M = 100 #num hidden nodes of the hidden layer\n",
    "        K = nameOfTargets.shape[0] #number of outputs\n",
    "\n",
    "        #creating the weights (randomly)\n",
    "        W = np.random.randn(D*M).reshape(D,M)\n",
    "        V = np.random.randn(M*K).reshape(M,K)\n",
    "\n",
    "\n",
    "        #creating the biased terms\n",
    "        b = np.random.randn(M).reshape(1,M) #Generating biased terms for hidden nodes\n",
    "        ones = np.ones(N).reshape(N,1) #Making an Nx1 matrix to multiply with the biased terms.\n",
    "        b = np.dot(b_ones,b) #After multiplying, NxM matrix (each row is a subject, each column is a node). The biased terms of each column are the same (biased term of each node does not change for the subject).\n",
    "\n",
    "        c = np.random.randn(K).reshape(1,K) #Repeating the process for the biased terms of the output layer. NxK matrix once completed.\n",
    "        c = np.dot(c_ones, c)\n",
    "\n",
    "        for j in range(iteration_num): #Back Propagation\n",
    "\n",
    "            #feed forward\n",
    "            z = np.dot(X_train,W) + b\n",
    "            z = 1/(1 + np.exp(-z.astype(float)))\n",
    "            predictions = np.exp(np.dot(z,V) + c)\n",
    "\n",
    "            #softmax\n",
    "            for i in range(predictions.shape[0]):\n",
    "                predictions[i,:] = predictions[i,:]/np.sum(predictions[i,:])\n",
    "\n",
    "\n",
    "            #gradient descent (all from formula)\n",
    "\n",
    "                #Calculating all the partial derivatives of cost\n",
    "            dV = np.dot(z.T,(y_train - predictions)) \n",
    "            dZ = np.dot(np.dot(np.dot((y_train - predictions), V.T).T, z),(1-z.T)) #will be used to calculate dW\n",
    "            dW = np.dot(X_train.T,dZ.T) \n",
    "            db = np.dot(np.dot(np.dot((y_train - predictions), V.T).T, z), (1-z.T)).T.sum(axis = 0) \n",
    "            dc = (y_train - predictions).sum(axis = 0) \n",
    "\n",
    "\n",
    "            W += a*dW.astype(float) #Gradient Descent\n",
    "            V += a*dV.astype(float) \n",
    "            b += a*db.astype(float)\n",
    "            c += a*dc.astype(float)\n",
    "\n",
    "            if j%100 == 0: #Every 100 iterations, print out the cost and accuracy\n",
    "                total = -np.dot(y_train.T, np.log(predictions))\n",
    "                cost = total.sum() #Cost of the model\n",
    "                Accuracy = np.mean(np.round(predictions) == y_train) #Accuracy of the model\n",
    "                print(cost, Accuracy)\n",
    "\n",
    "\n",
    "        print(\" \")\n",
    "        print(\"Final Cost and Accuracy of training data: \")\n",
    "        print(cost, Accuracy)\n",
    "\n",
    "        #Applying the model to the test data. The X_test data must be put through the softmax function and compared to y_test\n",
    "\n",
    "        #feed forward\n",
    "        z = np.dot(X_test,W) + b[0] #using only b causes a dimension error. Using b[0] gets all the relevant biased terms and makes use of how numpy array addition works.\n",
    "        z = 1/(1 + np.exp(-z.astype(float)))\n",
    "        test_predictions = np.exp(np.dot(z,V) + c[0]) #same thing with c[0]\n",
    "\n",
    "        #softmax\n",
    "        for i in range(test_predictions.shape[0]):\n",
    "            test_predictions[i,:] = test_predictions[i,:]/np.sum(test_predictions[i,:])\n",
    "\n",
    "        test_Acc = np.mean(np.round(test_predictions) == y_test)\n",
    "        test_total = -np.dot(y_test.T, np.log(test_predictions))\n",
    "        test_cost = total.sum()\n",
    "\n",
    "        print(\" \")\n",
    "        print(\"Cost and Accuracy of testing data: \")\n",
    "        print(test_cost, test_Acc)\n",
    "        \n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.W = W\n",
    "        self.V = V\n",
    "        self.nameOfTargets = nameOfTargets\n",
    "    def classify(self, data, X_new):\n",
    "    \"\"\"\n",
    "    X_new is an unstandardized copy of the original data\n",
    "    \"\"\"\n",
    "    for i in range(X_new.shape[1]):\n",
    "        data[:,i] = (data[:,i].astype(float) - np.mean(X_new[:,i].astype(float)))/np.std(X_new[:,i].astype(float)) \n",
    "        #Standardizing inputs using X_new, which was created earlier in the algorithm\n",
    "    \n",
    "    #Passing the inputs through feed forward\n",
    "    z = np.dot(data,self.W) + self.b[0]\n",
    "    z = 1/(1 + np.exp(-z.astype(float)))\n",
    "    test_predictions = np.exp(np.dot(z,self.V) + self.c[0])\n",
    "\n",
    "    #softmax\n",
    "    for i in range(test_predictions.shape[0]):\n",
    "        test_predictions[i,:] = test_predictions[i,:]/np.sum(test_predictions[i,:])\n",
    "    \n",
    "    test_predictions = np.round(test_predictions) #rounding the predictions to only get 1 and 0\n",
    "    j = np.where(test_predictions == 1)[1][0] #Getting the index of which target we classified the data as (which target node has a value of 1)\n",
    "    \n",
    "    \n",
    "    return nameOfTargets[j]#Returning the name of the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
